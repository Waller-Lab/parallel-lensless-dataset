{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lensless Dataset Tutorial\n",
    "This Jupyter notebook contains a tutorial for using the dataset from the [Parallel Lensless Dataset project](https://waller-lab.github.io/parallel-lensless-dataset/). All data can be found in and downloaded from [this folder](https://drive.google.com/drive/folders/1tY0lgakSKO-ztF5A_ulVPBQDTz4eh7T5?usp=drive_link). This notebook provides guidance using 8x downsampled and undistorted ground truth measurements that are warped to each lensless imager's coordinate spaces.\n",
    "\n",
    "Acronyms:\n",
    "- Ground Truth (GT)\n",
    "- Random Multi-Focal Lenslet (RML)\n",
    "- Diffusercam (DC)\n",
    "- Field-of-View (FOV)\n",
    "\n",
    "This notebook assumes that filenames are not changed within the downloaded data directory structure, and that all files are in a root directory. Please update the root directory path below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '' # TODO: Add the root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage.transform import rescale, resize\n",
    "import torch\n",
    "import kornia.geometry.transform as transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing ground truth measurements\n",
    "We have prepared [a folder](https://drive.google.com/drive/folders/1D8frfCtFoi3REklvPk--Tl55SCADH4PH?usp=drive_link) with pre-processed ground truth measurements. Starting with the full-resolution (1200, 1920) measurements, these measurements have been undistorted to correct for lens distortion, downsampled by a factor of 8 to form (150, 240) measurements, and warped to the space of each respective lensless imager.\n",
    "\n",
    "This section performs the following operations:\n",
    "- loads a single ground truth image from the dataset with shape (150, 240, 4)\n",
    "- removes the alpha channel associated with `matplotlib.imread`\n",
    "\n",
    "Download and unzip the files:\n",
    "- [GT to RML](https://drive.google.com/file/d/17ssGgNLTj8EKtvJozh3lO1hojNNJabK7/view?usp=sharing)\n",
    "- [GT to DC](https://drive.google.com/file/d/1HqKzfOOCtxrWYf0If1sK56EVVTrwMGxw/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ground Truth to RML\n",
    "object_idx = 64\n",
    "gt2RML_image_path = os.path.join(ROOT_DIR, 'undistorted_GT2RML', 'warped_undistorted_img_{}_cam_2.tiff'.format(object_idx))\n",
    "gt2RML_image = plt.imread(gt2RML_image_path)\n",
    "print(\"Image shape: \", gt2RML_image.shape) # verify that shape is (150, 240, 4)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Original GT2RML Image (150, 240)\")\n",
    "plt.imshow(gt2RML_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ground Truth to Diffusercam\n",
    "object_idx = 64\n",
    "gt2DC_image_path = os.path.join(ROOT_DIR, 'undistorted_GT2DC', 'warped_undistorted_img_{}_cam_2.tiff'.format(object_idx))\n",
    "gt2DC_image = plt.imread(gt2DC_image_path)\n",
    "print(\"Image shape: \", gt2DC_image.shape) # verify that shape is (150, 240, 4)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Original GT2DC Image (150, 240)\")\n",
    "plt.imshow(gt2DC_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing lensless imager measurements\n",
    "Lensless imager measurements need to be 8x downsampled to match the undistorted ground truth resolution: (1200, 1920) -> (150, 240)\n",
    "\n",
    "Download and unzip the files:\n",
    "- [Full resolution RML measurements](https://drive.google.com/file/d/16teCy-uPhz_NhC7DCmIbDiWkcVEZffV1/view?usp=sharing)\n",
    "- [Full resolution DC measurements](https://drive.google.com/file/d/10ODJ6n4sPufdOS1yr6KdjjjsMKZjnQqs/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling RML measurements\n",
    "object_idx = 64\n",
    "RML_image_path = os.path.join(ROOT_DIR, '/rml', 'img_{}_cam_1.tiff'.format(object_idx))\n",
    "RML_image = plt.imread(RML_image_path)\n",
    "print(\"Image shape: \", RML_image.shape) # verify that shape is (1200, 1920, 3)\n",
    "ds_RML_image = resize(RML_image, (150, 240), anti_aliasing=True).astype(np.float32) # resize to 150 x 240\n",
    "print(\"Resized shape: \", RML_image.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Original RML Measurement (1200, 1920)\")\n",
    "plt.imshow(RML_image)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Downsampled RML Measurement (150, 240)\")\n",
    "plt.imshow(ds_RML_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downsampling DC measurements\n",
    "object_idx = 64\n",
    "DC_image_path = os.path.join(ROOT_DIR, '/diffusercam', 'img_{}_cam_0.tiff'.format(object_idx))\n",
    "DC_image = plt.imread(DC_image_path)\n",
    "print(\"Image shape: \", DC_image.shape) # verify that shape is (1200, 1920, 3)\n",
    "ds_DC_image = resize(DC_image, (150, 240), anti_aliasing=True).astype(np.float32) # resize to 150 x 240\n",
    "print(\"Resized shape: \", DC_image.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Original DC Measurement (1200, 1920)\")\n",
    "plt.imshow(DC_image)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Downsampled DC Measurement (150, 240)\")\n",
    "plt.imshow(ds_DC_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing for training\n",
    "Once ground truth and lensless measurements have been loaded and processed, preprocessing may be necessary for training image reconstructions. For our image reconstruction algorithms in Pytorch, images are preprocessed as follows:\n",
    "- Ensure images are 3 channels\n",
    "- convert from uint8 to float32\n",
    "- resize image to correct downsampling level (may be redundant but is a good check). We use 8x downsampling (150, 240) but change this based on your downsampling level if you are using other data\n",
    "- normalize to 0 - 1 and clip image to 0-1\n",
    "- convert to a torch tensor\n",
    "- reshape to have channel dimension first (H, W, C) -> (C, H, W)\n",
    "\n",
    "In this example, we use an RML image to illustrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ds_RML_image\n",
    "target = gt2RML_image\n",
    "downsampled_dims = (150, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if images include alpha channel, remove\n",
    "if image.shape[-1] == 4:\n",
    "    image = image[..., :-1]\n",
    "if target.shape[-1] == 4:\n",
    "    target = target[..., :-1]\n",
    "\n",
    "# convert images to 0 to 1 range and convert to float, they're 8 bit 0 - 255 otherwise \n",
    "image = (image / 255.0).astype(np.float32)\n",
    "target = (target / 255.0).astype(np.float32) \n",
    "\n",
    "# resize measurement \n",
    "image = resize(image, downsampled_dims, anti_aliasing=True).astype(np.float32)\n",
    "assert image.shape == target.shape \n",
    "\n",
    "# clip 0-1\n",
    "image = np.clip(image, 0,1)\n",
    "target = np.clip(target, 0, 1)\n",
    "\n",
    "image = torch.from_numpy(image)\n",
    "target = torch.from_numpy(target)\n",
    "\n",
    "# Move channels to the front\n",
    "image = torch.moveaxis(image, -1, 0)\n",
    "target = torch.moveaxis(target, -1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cropping for training loss\n",
    "\n",
    "In training, you may want to crop out the black borders before loss evaluation. Since these reconstructions are done in each lensless imager's space, there is a different cropping FOV necessary. Below, we use examples of the crop regions we use for both RML and Diffusercam for (150, 240) reconstructions. \n",
    "We use the ground truth image in this example but you should replace the image with recons for each respective imager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rml recons \n",
    "rml_recon = gt2RML_image # placeholder, replace with your recon\n",
    "cropped_image = rml_recon[17:131, 64:178]\n",
    "plt.imshow(cropped_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diffusercam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dc recons \n",
    "dc_recon = gt2DC_image # placeholder, replace with your recon\n",
    "cropped_image = dc_recon[1:131, 59:189]\n",
    "plt.imshow(cropped_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation view\n",
    "In order to compare the three imagers (GT, DC, RML) in the same imaging space, we can warp both lensless imagers to the space of the ground truth camera and crop accordingly. To do this, we use a calibrated homography matrix and the Kornia package. We assume a single image, or batch size of 1, in this example.\n",
    "\n",
    "Download [both homography transform files here.](https://drive.google.com/drive/folders/19V4PdGSHDkb-5Dsu--KWWgOh2WOe7Rs4?usp=drive_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to homography and load homography:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rml_homog_path = os.path.join(ROOT_DIR, 'Lensless2GroundTruth') #Lensless2GroundTruth\n",
    "rml_homography = torch.load(os.path.join(rml_homog_path, 'RML2GT_homography_8x_color_detached.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load recons and apply homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recons need to be in b x c x h x w format \n",
    "rml_recon_path = '' # TODO: Add path to your recons\n",
    "rml_recon = np.load(rml_recon_path)\n",
    "rml_recon_tensor = torch.from_numpy(rml_recon) \n",
    "rml_recon_tensor = transform.warp_perspective(rml_recon_tensor, rml_homography, dsize=(150, 240), align_corners=True)\n",
    "cropped_rml_recon = rml_recon_tensor[25:127, 63:165]\n",
    "plt.imshow(cropped_rml_recon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for the diffusercam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_homog_path = os.path.join(ROOT_DIR, 'Lensless2GroundTruth')\n",
    "dc_homography = torch.load(os.path.join(dc_homog_path, 'DC2GT_homography_8x_color_detached.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recons need to be in b x c x h x w format \n",
    "dc_recon_path = '' # TODO: Add path to your recons\n",
    "dc_recon = np.load(dc_recon_path)\n",
    "dc_recon_tensor = torch.from_numpy(dc_recon) \n",
    "dc_recon_tensor = transform.warp_perspective(dc_recon_tensor, dc_homography, dsize=(150, 240), align_corners=True)\n",
    "cropped_dc_recon = dc_recon_tensor[25:127, 63:165]\n",
    "plt.imshow(cropped_dc_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
